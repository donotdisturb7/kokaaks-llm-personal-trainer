# Backend - KovaaK's AI Personal Trainer

Backend FastAPI pour l'assistant IA d'entraÃ®nement de visÃ©e avec connexion modulaire Ã  Ollama.

## ğŸš€ DÃ©marrage rapide

### 1. Installation des dÃ©pendances

```bash
cd backend
pip install -r requirements.txt
```

### 2. Configuration

Copiez le fichier d'exemple et configurez vos paramÃ¨tres :

```bash
cp env.example .env
```

Ã‰ditez le fichier `.env` pour configurer :

```env
# Configuration Ollama - modulaire pour localhost ou IP
OLLAMA_HOST=localhost          # ou une IP distante
OLLAMA_PORT=11434
OLLAMA_MODEL=llama2           # ou votre modÃ¨le prÃ©fÃ©rÃ©
OLLAMA_TIMEOUT=30

# Configuration API
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=true
```

### 3. Test de connexion Ollama

```bash
python test_ollama.py
```

### 4. DÃ©marrage de l'API

```bash
python run.py
```

L'API sera accessible sur : http://localhost:8000
Documentation : http://localhost:8000/docs

## ğŸ”§ Configuration Ollama

### Connexion locale
```env
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
```

### Connexion distante
```env
OLLAMA_HOST=192.168.1.100    # IP de votre serveur Ollama
OLLAMA_PORT=11434
```

## ğŸ“¡ Endpoints API

### Chat avec l'IA
- `POST /api/chat/message` - Envoyer un message
- `POST /api/chat/conversation` - Conversation complÃ¨te
- `GET /api/chat/health` - VÃ©rifier la connexion Ollama
- `GET /api/chat/models` - Lister les modÃ¨les disponibles

### SantÃ© de l'API
- `GET /health` - Statut de l'API
- `GET /` - Informations gÃ©nÃ©rales

## ğŸ—ï¸ Architecture

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # Application FastAPI
â”‚   â”œâ”€â”€ config.py            # Configuration et variables d'env
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ chat.py          # Routes pour le chat
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ ollama_service.py # Service Ollama modulaire
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ env.example
â”œâ”€â”€ run.py                   # Script de lancement
â”œâ”€â”€ test_ollama.py          # Test de connexion
â””â”€â”€ README.md
```

## ğŸ” FonctionnalitÃ©s

- âœ… Connexion modulaire Ollama (localhost/IP)
- âœ… API REST avec FastAPI
- âœ… Gestion des erreurs
- âœ… Logging configurÃ©
- âœ… CORS configurÃ©
- âœ… Documentation automatique
- âœ… Tests de connexion
- âœ… Conseils spÃ©cialisÃ©s aim training

## ğŸ› DÃ©pannage

### Ollama non accessible
1. VÃ©rifiez qu'Ollama est dÃ©marrÃ© : `ollama serve`
2. VÃ©rifiez la configuration dans `.env`
3. Testez la connexion : `python test_ollama.py`

### ModÃ¨le non trouvÃ©
1. TÃ©lÃ©chargez un modÃ¨le : `ollama pull llama2`
2. VÃ©rifiez les modÃ¨les disponibles : `ollama list`

### Erreurs de connexion
1. VÃ©rifiez le firewall
2. VÃ©rifiez l'IP et le port
3. Testez avec curl : `curl http://localhost:11434/api/tags`
